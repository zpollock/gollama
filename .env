#The path contianing your model.  It will be mounted to the docker container at /usr/src/app/chat/models.
LLAMA_MODEL_PATH=../llama/7B/
#The model that will be loaded.
LLAMA_MODEL=ggml-model-q4_0.bin
#The llama.cpp server command and settings.  Configure the options for your system requirements.  
LLAMA_CPP_SERVER="./server --threads 8 --ctx_size 2048 --batch-size 256 --rope-freq-scale 1.0"
#The port for the gin API
PORT=8081
